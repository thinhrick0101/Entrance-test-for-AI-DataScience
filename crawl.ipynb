{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "import requests\n",
    "import zipfile\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract():\n",
    "    \"\"\"\n",
    "    Downloads a ZIP file containing legal documents from the given URL,\n",
    "    extracts its contents, and deletes the ZIP file afterward.\n",
    "    \"\"\"\n",
    "    url = \"https://phapdien.moj.gov.vn/TraCuuPhapDien/Files/BoPhapDienDienTu.zip\"\n",
    "    zip_file = 'BoPhapDienDienTu.zip'\n",
    "    extract_to = 'BoPhapDienDienTu'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    with open(zip_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "        \n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    os.remove(zip_file)\n",
    "    logging.info(\"ZIP file downloaded and extracted successfully\")\n",
    "\n",
    "download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'BoPhapDienDienTu'\n",
    "subdirs = ['vbpl', 'property', 'history', 'related', 'pdf']\n",
    "for sub in subdirs:\n",
    "    os.makedirs(os.path.join(base_dir, sub), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_with_retry(session, url, retries=3, delay=1):\n",
    "    \"\"\"\n",
    "    Asynchronously fetches data from a given URL with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        session (aiohttp.ClientSession): The active HTTP session.\n",
    "        url (str): The target URL to fetch data from.\n",
    "        retries (int, optional): Number of retry attempts. Defaults to 3.\n",
    "        delay (int, optional): Delay in seconds between retries. Defaults to 1.\n",
    "    \n",
    "    Returns:\n",
    "        str or bytes: The response text or binary data, or None on failure.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with session.get(url) as response:\n",
    "                if response.status == 200:\n",
    "                    return await response.read() if 'pdf' in url else await response.text()\n",
    "                elif response.status == 429:\n",
    "                    await asyncio.sleep(delay * (attempt + 1))\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                logging.error(f\"Failed to fetch {url}: {e}\")\n",
    "                return None\n",
    "            await asyncio.sleep(delay)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def save_content(content, save_path):\n",
    "    \"\"\"\n",
    "    Asynchronously saves content to a file.\n",
    "    \n",
    "    Args:\n",
    "        content (str or bytes): The content to be saved. If None, the function exits.\n",
    "        save_path (str): The file path where the content will be saved.\n",
    "    \"\"\"\n",
    "    if content is None:\n",
    "        return\n",
    "    try:\n",
    "        mode = 'wb' if isinstance(content, bytes) else 'w'\n",
    "        encoding = None if isinstance(content, bytes) else 'utf-8'\n",
    "        async with aiofiles.open(save_path, mode=mode, encoding=encoding) as f:\n",
    "            await f.write(content)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving {save_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_item_id(session, item_id):\n",
    "    \"\"\"\n",
    "    Asynchronously processes a single item ID by fetching and saving its content.\n",
    "    \n",
    "    Args:\n",
    "        session (aiohttp.ClientSession): The active HTTP session.\n",
    "        item_id (str): The item ID to process.\n",
    "    \"\"\"\n",
    "    urls = {\n",
    "        'vbpl': f\"https://vbpl.vn/TW/Pages/vbpq-toanvan.aspx?ItemID={item_id}&Keyword=\",\n",
    "        'property': f\"https://vbpl.vn/tw/Pages/vbpq-thuoctinh.aspx?dvid=13&ItemID={item_id}&Keyword=\",\n",
    "        'history': f\"https://vbpl.vn/tw/Pages/vbpq-lichsu.aspx?dvid=13&ItemID={item_id}&Keyword=\",\n",
    "        'related': f\"https://vbpl.vn/TW/Pages/vbpq-vanbanlienquan.aspx?ItemID={item_id}&Keyword=\",\n",
    "        'pdf': f\"https://vbpl.vn/tw/Pages/vbpq-van-ban-goc.aspx?ItemID={item_id}\"\n",
    "    }\n",
    "    \n",
    "    tasks = []\n",
    "    for doc_type, url in urls.items():\n",
    "        prefix = {'vbpl': 'full_', 'property': 'p_', 'history': 'h_', 'related': 'r_', 'pdf': 'pdf_'}[doc_type]\n",
    "        ext = '.pdf' if doc_type == 'pdf' else '.html'\n",
    "        save_path = os.path.join(base_dir, doc_type, f\"{prefix}{item_id}{ext}\")\n",
    "        \n",
    "        if not os.path.exists(save_path):\n",
    "            content = await fetch_with_retry(session, url)\n",
    "            if content:\n",
    "                tasks.append(save_content(content, save_path))\n",
    "    \n",
    "    if tasks:\n",
    "        await asyncio.gather(*tasks)\n",
    "        await asyncio.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_index_file(session, index_path):\n",
    "    \"\"\"\n",
    "    Asynchronously processes an index file by extracting item IDs and processing each one.\n",
    "    \n",
    "    Args:\n",
    "        session (aiohttp.ClientSession): The active HTTP session.\n",
    "        index_path (str): The path to the index file to process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        async with aiofiles.open(index_path, 'r', encoding='utf-8') as f:\n",
    "            content = await f.read()\n",
    "        \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        item_ids = set()\n",
    "        for link in soup.find_all('a', href=re.compile(r\"ItemID=\")):\n",
    "            match = re.search(r\"ItemID=(\\d+)\", link['href'])\n",
    "            if match:\n",
    "                item_ids.add(match.group(1))\n",
    "        \n",
    "        tasks = [process_item_id(session, item_id) for item_id in item_ids]\n",
    "        await asyncio.gather(*tasks)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {index_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \"\"\"\n",
    "    Asynchronous entry point for the crawler.\n",
    "    \n",
    "    This function processes all index files in the 'demuc' directory.\n",
    "    \"\"\"\n",
    "    demuc_dir = os.path.join(base_dir, \"demuc\")\n",
    "    index_files = [f for f in os.listdir(demuc_dir) if f.endswith(\".html\")]\n",
    "    \n",
    "    connector = aiohttp.TCPConnector(limit=5)\n",
    "    timeout = aiohttp.ClientTimeout(total=300)\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "        for index_file in tqdm(index_files, desc=\"Processing index files\"):\n",
    "            await process_index_file(session, os.path.join(demuc_dir, index_file))\n",
    "\n",
    "# Run the crawler\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
